{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data\n",
    "from scipy import misc\n",
    "from torch import optim\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from collections import namedtuple\n",
    "import torchvision.models as models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from laserembeddings import Laser\n",
    "\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable gpu device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 8888\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training label\n",
    "train_df_fake = pd.read_csv('~/adversarial_learning/weibo/tweets/train_rumor.txt', sep='|', header = None)\n",
    "train_df_real = pd.read_csv('~/adversarial_learning/weibo/tweets/train_nonrumor.txt', sep='|', header = None)\n",
    "# load test label\n",
    "test_df_fake = pd.read_csv('~/adversarial_learning/weibo/tweets/test_rumor.txt', sep='|', header = None)\n",
    "test_df_real = pd.read_csv('~/adversarial_learning/weibo/tweets/test_nonrumor.txt', sep='|', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_df_real.iterrows():\n",
    "    for col in range(1, 10):\n",
    "        if not pd.isna(row[col]) and 'sinaimg' in row[col]:\n",
    "            test_df_real.at[index, 0] = test_df_real.at[index, 0] + ',' + row[col]\n",
    "#             row[0] = row[0] + ',' + row[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_df_real.iterrows():\n",
    "    for col in range(1, 10):\n",
    "        if not pd.isna(row[col]) and 'sinaimg' in row[col]:\n",
    "            train_df_real.at[index, 0] = train_df_real.at[index, 0] + ',' + row[col]\n",
    "#             row[0] = row[0] + ',' + row[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news text\n",
    "train_fake = train_df_fake[0].tolist()\n",
    "train_real = train_df_real[0].tolist()\n",
    "test_fake = test_df_fake[0].tolist()\n",
    "test_real = test_df_real[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2934"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure each news has 3 lines\n",
    "def fix_offset(list_):\n",
    "    fixed_flag = False\n",
    "\n",
    "    while not fixed_flag:\n",
    "        exit_flag=False\n",
    "        temp = copy.deepcopy(list_)\n",
    "        for i,v in enumerate(temp):\n",
    "            if v!=None:\n",
    "                if 'sinaimg.cn' in v:\n",
    "                    if list_[i+1] !=None:\n",
    "                        if list_[i+1].isdigit():\n",
    "                            list_.insert(i+1,None)\n",
    "                            exit_flag=True\n",
    "                            break\n",
    "        if not exit_flag:\n",
    "            fixed_flag=True\n",
    "            \n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake = fix_offset(train_fake) \n",
    "train_real = fix_offset(train_real)\n",
    "test_fake = fix_offset(test_fake)\n",
    "test_real = fix_offset(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11244"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 lines -> 1 news\n",
    "def break_in_block(list_):\n",
    "    temp = []\n",
    "    for i in range(0,len(list_),3):\n",
    "        temp.append(list_[i:i+3])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake = break_in_block(train_fake)\n",
    "train_real = break_in_block(train_real)\n",
    "test_fake = break_in_block(test_fake)\n",
    "test_real = break_in_block(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3748, 3783, 1000, 996)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fake),len(train_real),len(test_fake),len(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_text_list(blocks_list):\n",
    "    image_list = []\n",
    "    text_list = []\n",
    "    for i in blocks_list:\n",
    "        if i[-1] !=None:\n",
    "            image_list.append(i[1])\n",
    "            text_list.append(i[-1])\n",
    "#     image_list = [i.split('/')[-1] for i in image_list]\n",
    "    image_list = [linklist.split(',') for linklist in image_list]\n",
    "    return image_list, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake_image,train_fake_text = get_image_and_text_list(train_fake)\n",
    "train_real_image,train_real_text = get_image_and_text_list(train_real)\n",
    "test_fake_image,test_fake_text = get_image_and_text_list(test_fake)\n",
    "test_real_image,test_real_text = get_image_and_text_list(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_real_image[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3698"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_fake_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_fake_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake_Y = [1]*len(train_fake_image)\n",
    "train_real_Y = [0]*len(train_real_image)\n",
    "test_fake_Y = [1]*len(test_fake_image)\n",
    "test_real_Y = [0]*len(test_real_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_fake_image+train_real_image\n",
    "train_text = train_fake_text + train_real_text\n",
    "trainY = train_fake_Y+train_real_Y\n",
    "\n",
    "test_images = test_fake_image+test_real_image\n",
    "test_text = test_fake_text+test_real_text\n",
    "testY = test_fake_Y+test_real_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7481, 7481, 7481, 1930, 1930, 1930)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images),len(train_text),len(trainY),len(test_images),len(test_text),len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images, dtype=object)\n",
    "train_text = np.array(train_text)\n",
    "trainY = np.array(trainY)\n",
    "test_images = np.array(test_images, dtype=object)\n",
    "test_text = np.array(test_text)\n",
    "testY = np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_delete(list_):\n",
    "    list_images_dir = listdir('../../../weibo/images')\n",
    "    gif_list = ['957e1cf2tw1e5foxts295g206o03p4qp.gif','a716fd45jw1ev0cgf8j46g209505zh4i.gif','005vnhZYgw1evupo8ttddg308w06o4qp.gif','7da75521gw1ele2jvi85rg2096056u0x.gif']\n",
    "    index = []\n",
    "    image_name_list = []\n",
    "    for i,vlist in enumerate(list_):\n",
    "        exclude = True\n",
    "        for v in vlist:\n",
    "            vname = v.split('/')[-1]\n",
    "            if vname in list_images_dir:\n",
    "                image_name_list.append(vname)\n",
    "                exclude = False\n",
    "                break\n",
    "            if vname in gif_list:\n",
    "                image_name_list.append(vname)\n",
    "                exclude = True\n",
    "                break\n",
    "        if exclude:\n",
    "            image_name_list.append(\"excluded\")\n",
    "            index.append(i)\n",
    "                \n",
    "    return index, image_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_delete_index, train_images =index_to_delete(train_images)\n",
    "test_delete_index, test_images = index_to_delete(test_images)\n",
    "len(train_delete_index)+len(test_delete_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delete_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.delete(train_images,train_delete_index)\n",
    "train_text = np.delete(train_text,train_delete_index)\n",
    "trainY = np.delete(trainY,train_delete_index)\n",
    "test_images = np.delete(test_images,test_delete_index)\n",
    "test_text = np.delete(test_text,test_delete_index)\n",
    "testY = np.delete(testY,test_delete_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2807"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real news im train\n",
    "(trainY==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3347"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fake news in train\n",
    "(trainY==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(testY==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(testY==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index= np.arange(len(train_images))\n",
    "np.random.shuffle(shuffle_index)\n",
    "train_images = train_images[shuffle_index]\n",
    "train_text = train_text[shuffle_index]\n",
    "trainY = trainY[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1370, 4821, 4817, ..., 2120, 3299, 4483])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6154, 6154, 6154, 1699, 1699, 1699)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images),len(train_text),len(trainY),len(test_images),len(test_text),len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6154,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_size = 128\n",
    "# pretrained_size = 256\n",
    "pretrained_size = 224\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ])\n",
    "\n",
    "test_transforms = train_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip pretrained model for image encoding\n",
    "\n",
    "clip_pretrained = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# Laser model for text encoding\n",
    "\n",
    "laser_model = Laser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeiboDataset(Dataset):\n",
    "    \"\"\"Weibo dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, images, labels, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (np.array): train texts\n",
    "            images (np.array): train images\n",
    "            labels (np.array): train labels\n",
    "            root_dir (string): root directory of images\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                to the image.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.texts.shape[0]\n",
    "    \n",
    "    def read_and_process_image(self, list_of_images):\n",
    "        X = [] \n",
    "        for image in tqdm(list_of_images):\n",
    "            X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (length,width), interpolation=cv2.INTER_CUBIC))  \n",
    "        return X\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.images[idx])  # \"image_id(s)\"\n",
    "        image = Image.open(img_name)#.convert(\"RGB\")   # convert to RGB is important\n",
    "        image = torch.Tensor(clip_pretrained.encode(image))\n",
    "        label = self.labels[idx]   # label\n",
    "        \n",
    "        text = self.texts[idx]   # post_text\n",
    "        text = torch.Tensor(laser_model.embed_sentences(text, lang='zh'))\n",
    "        \n",
    "        sample = {'news_image': image, 'label': label, 'news_text': text}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['news_image'] = self.transform(news_image)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeiboTestset(Dataset):\n",
    "    \"\"\"Twitter dataset testset.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, images, labels, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (np.array): train texts\n",
    "            images (np.array): train images\n",
    "            labels (np.array): train labels\n",
    "            root_dir (string): root directory of images\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                to the image.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.texts.shape[0]\n",
    "    \n",
    "    def read_and_process_image(self, list_of_images):\n",
    "        X = [] \n",
    "        for image in tqdm(list_of_images):\n",
    "            X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (length,width), interpolation=cv2.INTER_CUBIC))  \n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.images[idx])  # \"image_id(s)\"\n",
    "        image = Image.open(img_name)#.convert(\"RGB\")   # convert to RGB is important\n",
    "        image = torch.Tensor(clip_pretrained.encode(image))\n",
    "        label = self.labels[idx]   # label\n",
    "        \n",
    "        text = self.texts[idx]   # post_text\n",
    "        text = torch.Tensor(laser_model.embed_sentences(text, lang='zh'))\n",
    "        \n",
    "        sample = {'news_image': image, 'label': label, 'news_text': text}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['news_image'] = self.transform(news_image)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the train and trial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../../weibo/images'\n",
    "train_data = WeiboDataset(train_text, train_images, trainY, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = WeiboTestset(test_text, test_images, testY, root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID_RATIO = 0.9\n",
    "\n",
    "# n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "# n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "# train_data, valid_data = data.random_split(train_data, \n",
    "#                                            [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6154\n",
      "Number of testing examples: 1699\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gate Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateModule(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    A simple gate implemented with\n",
    "    - a linear layer\n",
    "    - a sigmoid layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_dim):\n",
    "        super(GateModule, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.gate = nn.Linear(in_dim, 1)\n",
    "        self.prob = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        score = self.gate(x)\n",
    "        score = self.prob(score)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct VAE class\n",
    "- gate is applied only to the text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    Gate is applied only to the text input\n",
    "    '''\n",
    "    def __init__(self, zsize, output_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zsize = zsize\n",
    "        self.fc1 = nn.Linear(zsize, zsize)   # 4 * 4 is the current size of the image\n",
    "        self.fc2 = nn.Linear(zsize, zsize)\n",
    "\n",
    "        ######\n",
    "        # multi-tasks sub-networks\n",
    "        self.fc_news = nn.Linear(zsize, output_dim)\n",
    "        self.fc_shaming = nn.Linear(zsize, output_dim)\n",
    "        self.fc_stereotype = nn.Linear(zsize, output_dim)\n",
    "        self.fc_objectification = nn.Linear(zsize, output_dim)\n",
    "        self.fc_violence = nn.Linear(zsize, output_dim)\n",
    "        \n",
    "        \n",
    "        # encoder layers\n",
    "        self.enc_txt_fc = nn.Linear(1024, int(0.5 * zsize))\n",
    "        self.enc_img_fc1 = nn.Linear(512, int(0.5 * zsize))\n",
    "#         self.enc_img_fc2 = nn.Linear(1024, int(0.5 * zsize))\n",
    "        \n",
    "        # decoder layers\n",
    "        self.dec_txt_fc = nn.Linear(zsize, 1024)\n",
    "        self.dec_img_fc1 = nn.Linear(zsize, 512)\n",
    "#         self.dec_img_fc2 = nn.Linear(1024, 2048)\n",
    "\n",
    "        # batch normalizations\n",
    "        self.enc_txt_bn = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "        self.enc_img_bn1 = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "#         self.enc_img_bn2 = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "        \n",
    "        self.dec_txt_bn = nn.BatchNorm1d(num_features=1024)\n",
    "        self.dec_img_bn1 = nn.BatchNorm1d(num_features=512)\n",
    "#         self.dec_img_bn2 = nn.BatchNorm1d(num_features=2048)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout_txt_enc = nn.Dropout(0.2)\n",
    "        self.dropout_img_enc = nn.Dropout(0.2)\n",
    "        self.dropout_txt_dec = nn.Dropout(0.2)\n",
    "        self.dropout_img_dec = nn.Dropout(0.2)\n",
    "        \n",
    "        # gated unit\n",
    "        self.gated_unit = GateModule(1024)  # text embedding dimension\n",
    "        \n",
    "        \n",
    "    def img_encode(self, x_img):\n",
    "#         _, x_img = self.resnet_pretrained(x_img)\n",
    "        x_img = F.relu(self.dropout_img_enc(self.enc_img_bn1(self.enc_img_fc1(x_img))))\n",
    "#         x_img = F.relu(self.enc_img_fc2(x_img))\n",
    "        \n",
    "        return x_img   # [bs, 2048]\n",
    "\n",
    "    def txt_encode(self, x_txt):\n",
    "\n",
    "        gate = self.gated_unit(x_txt)\n",
    "        x_txt = x_txt * gate\n",
    "        x_txt = x_txt.view(x_txt.shape[0], 1024)\n",
    "        x_txt = F.relu(self.dropout_txt_enc(self.enc_txt_bn(self.enc_txt_fc(x_txt))))\n",
    "        return x_txt, gate   # [bs, 0.5 * zsize]\n",
    "\n",
    "    def encode(self, x_img, x_txt):\n",
    "        \n",
    "        x_img = self.img_encode(x_img)\n",
    "        \n",
    "        x_txt, gate = self.txt_encode(x_txt)\n",
    "        \n",
    "        # concate x_img and x_txt\n",
    "        x = torch.cat((x_txt, x_img), 1)\n",
    "        \n",
    "        h1 = self.fc1(x)   # mu\n",
    "        h2 = self.fc2(x)   # logvar\n",
    "        return h1, h2, gate\n",
    "    \n",
    "    def subtask_news(self, z):\n",
    "        \n",
    "        h = self.fc_news(z)\n",
    "        return h\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, x):\n",
    "#         x = x.view(x.shape[0], self.zsize)   # flatten\n",
    "\n",
    "        # Decoding txt\n",
    "        dec_x_txt = F.relu(self.dropout_txt_dec(self.dec_txt_bn(self.dec_txt_fc(x))))\n",
    "        \n",
    "        # Decoding img\n",
    "        dec_x_img = F.relu(self.dropout_img_dec(self.dec_img_bn1(self.dec_img_fc1(x))))\n",
    "#         dec_x_img = F.relu(self.dec_img_fc2(dec_x_img))\n",
    "        \n",
    "        return dec_x_img, dec_x_txt\n",
    "\n",
    "    def forward(self, x_img, x_txt):\n",
    "        mu, logvar, p = self.encode(x_img, x_txt)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        y_news = self.subtask_news(z)\n",
    "        \n",
    "        y_pred = dict()\n",
    "        y_pred[\"news\"] = y_news\n",
    "        \n",
    "        dec_x_img, dec_x_txt = self.decode(z.view(-1, self.zsize))\n",
    "        \n",
    "        return dec_x_img, dec_x_txt, mu, logvar, y_pred, p\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gate is applied to both the text input & image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Gate is applied to both the text input & image input\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, zsize, output_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zsize = zsize\n",
    "        self.fc1 = nn.Linear(zsize, zsize)   # 4 * 4 is the current size of the image\n",
    "        self.fc2 = nn.Linear(zsize, zsize)\n",
    "\n",
    "        ######\n",
    "        # multi-tasks sub-networks\n",
    "        self.fc_news = nn.Linear(zsize, output_dim)\n",
    "        self.fc_domain = nn.Linear(zsize, 15)\n",
    "        \n",
    "        \n",
    "        # encoder layers\n",
    "        self.enc_txt_fc = nn.Linear(1024, int(0.5 * zsize))\n",
    "        self.enc_img_fc1 = nn.Linear(512, int(0.5 * zsize))\n",
    "#         self.enc_img_fc2 = nn.Linear(1024, int(0.5 * zsize))\n",
    "        \n",
    "        # decoder layers\n",
    "        self.dec_txt_fc = nn.Linear(zsize, 1024)\n",
    "        self.dec_img_fc1 = nn.Linear(zsize, 512)\n",
    "#         self.dec_img_fc2 = nn.Linear(1024, 2048)\n",
    "\n",
    "        # batch normalizations\n",
    "        self.enc_txt_bn = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "        self.enc_img_bn1 = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "#         self.enc_img_bn2 = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "        \n",
    "        self.dec_txt_bn = nn.BatchNorm1d(num_features=1024)\n",
    "        self.dec_img_bn1 = nn.BatchNorm1d(num_features=512)\n",
    "#         self.dec_img_bn2 = nn.BatchNorm1d(num_features=2048)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout_txt_enc = nn.Dropout(0.2)\n",
    "        self.dropout_img_enc = nn.Dropout(0.2)\n",
    "        self.dropout_txt_dec = nn.Dropout(0.2)\n",
    "        self.dropout_img_dec = nn.Dropout(0.2)\n",
    "        \n",
    "        # gated unit\n",
    "        self.gate = GateModule(zsize)  # text embedding dimension\n",
    "        \n",
    "        \n",
    "    def img_encode(self, x_img):\n",
    "#         _, x_img = self.resnet_pretrained(x_img)\n",
    "        x_img = F.relu(self.dropout_img_enc(self.enc_img_bn1(self.enc_img_fc1(x_img))))\n",
    "#         x_img = F.relu(self.enc_img_fc2(x_img))\n",
    "        \n",
    "        return x_img   # [bs, 2048]\n",
    "\n",
    "    def txt_encode(self, x_txt):\n",
    "\n",
    "        x_txt = x_txt.view(x_txt.shape[0], 1024)\n",
    "        x_txt = F.relu(self.dropout_txt_enc(self.enc_txt_bn(self.enc_txt_fc(x_txt))))\n",
    "        return x_txt  # [bs, 0.5 * zsize]\n",
    "\n",
    "    def encode(self, x_img, x_txt):\n",
    "        \n",
    "        x_img = self.img_encode(x_img)\n",
    "        \n",
    "        x_txt = self.txt_encode(x_txt)\n",
    "        \n",
    "        # concate x_img and x_txt\n",
    "        x = torch.cat((x_txt, x_img), 1)\n",
    "        \n",
    "        p = self.gate(x)\n",
    "        \n",
    "        x = torch.cat((p * x_txt, (1 - p) * x_img), 1)\n",
    "        \n",
    "        h1 = self.fc1(x)   # mu\n",
    "        h2 = self.fc2(x)   # logvar\n",
    "        return h1, h2, p\n",
    "    \n",
    "    def subtask_news(self, z):\n",
    "        \n",
    "        h = self.fc_news(z)\n",
    "        return h\n",
    "    \n",
    "    def subtask_domain(self, z):\n",
    "        \n",
    "        h = self.fc_domain(z)\n",
    "        return h\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, x):\n",
    "#         x = x.view(x.shape[0], self.zsize)   # flatten\n",
    "\n",
    "        # Decoding txt\n",
    "        dec_x_txt = F.relu(self.dropout_txt_dec(self.dec_txt_bn(self.dec_txt_fc(x))))\n",
    "        \n",
    "        # Decoding img\n",
    "        dec_x_img = F.relu(self.dropout_img_dec(self.dec_img_bn1(self.dec_img_fc1(x))))\n",
    "#         dec_x_img = F.relu(self.dec_img_fc2(dec_x_img))\n",
    "        \n",
    "        return dec_x_img, dec_x_txt\n",
    "\n",
    "    def forward(self, x_img, x_txt):\n",
    "        mu, logvar, p = self.encode(x_img, x_txt)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        y_news = self.subtask_news(z)\n",
    "        y_domain = self.subtask_domain(z)\n",
    "        \n",
    "        y_pred = dict()\n",
    "        y_pred[\"news\"] = y_news\n",
    "        y_pred[\"domain\"] = y_domain\n",
    "        \n",
    "        dec_x_img, dec_x_txt = self.decode(z.view(-1, self.zsize))\n",
    "        \n",
    "        return dec_x_img, dec_x_txt, mu, logvar, y_pred, p\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x_img, recon_x_txt, x_img, x_txt, mu, logvar):\n",
    "    BCE_img = torch.mean((recon_x_img - x_img)**2)\n",
    "    BCE_txt = torch.mean((recon_x_txt - x_txt)**2)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(torch.mean(1 + logvar - mu.pow(2) - logvar.exp(), 1))\n",
    "    return BCE_img, BCE_txt, KLD * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"VAE_weibo_p(1-p)_gated_result.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    name_dict = dict()\n",
    "    name_dict[\"news\"] = 0\n",
    "#     name_dict[\"shaming\"] = 1\n",
    "#     name_dict[\"stereotype\"] = 2\n",
    "#     name_dict[\"objectification\"] = 3\n",
    "#     name_dict[\"violence\"] = 4\n",
    "    \n",
    "    #batch_size = 32\n",
    "    z_size = 512\n",
    "#     z_size = 1024\n",
    "    vae = VAE(z_size)\n",
    "    vae.cuda()\n",
    "    vae.train()\n",
    "    vae.weight_init(mean=0, std=0.02)\n",
    "\n",
    "    lr = 0.0001\n",
    "\n",
    "    vae_optimizer = optim.Adam(vae.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    " \n",
    "    train_epoch = 30\n",
    "\n",
    "    \n",
    "    dataloader = train_iterator\n",
    "    \n",
    "    f1_max = 0\n",
    "    max_acc = 0\n",
    "    \n",
    "    for epoch in range(train_epoch):\n",
    "        vae.train()\n",
    "\n",
    "        rec_txt_loss = 0\n",
    "        rec_img_loss = 0\n",
    "        kl_loss = 0\n",
    "        subtask_news_loss = 0\n",
    "#         subtask_shaming_loss = 0\n",
    "#         subtask_stereotype_loss = 0\n",
    "#         subtask_objectification_loss = 0\n",
    "#         subtask_violence_loss = 0\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        if (epoch + 1) % 8 == 0:\n",
    "            vae_optimizer.param_groups[0]['lr'] /= 4\n",
    "#             print(\"learning rate change!\")\n",
    "            f.write(\"learning rate change! The learning rate is %1.4f now\\n\" % (lr))\n",
    "\n",
    "#         i = 0\n",
    "        acc = 0\n",
    "        num = 0\n",
    "        for i, data in tqdm(enumerate(dataloader, 0), desc='iterations'):\n",
    "        #for x in batches:\n",
    "            vae.train()\n",
    "            \n",
    "            #inputs, classes = data\n",
    "            img_inputs = data['news_image']\n",
    "            img_inputs = img_inputs.to(device)\n",
    "\n",
    "            txt_inputs = data[\"news_text\"]\n",
    "            txt_inputs = txt_inputs.to(device)\n",
    "            \n",
    "            classes = data['label']\n",
    "            \n",
    "            # multi-task labels\n",
    "            classes_news = classes\n",
    "            \n",
    "            img_inputs, txt_inputs, classes_news = Variable(img_inputs), Variable(txt_inputs), Variable(classes_news)\n",
    "        \n",
    "            img_inputs = img_inputs.to(device)\n",
    "            txt_inputs = txt_inputs.to(device)\n",
    "            classes_news = classes_news.to(device)\n",
    "            \n",
    "            vae.zero_grad()\n",
    "#             rec, mu, logvar = vae(x)\n",
    "            rec_img, rec_txt, mu, logvar, y_pred, p = vae(img_inputs, txt_inputs)\n",
    "\n",
    "            loss_re_img, loss_re_txt, loss_kl = loss_function(rec_img, rec_txt, img_inputs, txt_inputs, mu, logvar)\n",
    "            loss_subtask_news = criterion(y_pred[\"news\"], classes_news)\n",
    "            \n",
    "            (loss_re_img + loss_re_txt + loss_kl + loss_subtask_news).backward()\n",
    "            \n",
    "            vae_optimizer.step()\n",
    "            rec_img_loss += loss_re_img.item()\n",
    "            rec_txt_loss += loss_re_txt.item()\n",
    "            \n",
    "            kl_loss += loss_kl.item()\n",
    "            subtask_news_loss += loss_subtask_news.item()\n",
    "            \n",
    "            # Calculate batch accuracy\n",
    "            _, top_pred = y_pred[\"news\"].topk(1, 1)\n",
    "            y = classes_news.cpu()\n",
    "            batch_size = y.shape[0]\n",
    "            top_pred = top_pred.cpu().view(batch_size)\n",
    "            acc += sum(top_pred == y).item()\n",
    "            num += batch_size\n",
    "\n",
    "            #############################################\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "\n",
    "            # report losses and save samples each 60 iterations\n",
    "            m = len(dataloader)\n",
    "            i += 1\n",
    "            if i % m == 0:\n",
    "                rec_txt_loss /= m\n",
    "                rec_img_loss /= m\n",
    "                kl_loss /= m\n",
    "                subtask_news_loss /= m\n",
    "\n",
    "                f.write('\\n[%d/%d] - ptime: %.2f, rec img loss: %.9f, rec txt loss: %.9f, KL loss: %.9f, news loss: %.9f\\n' % (\n",
    "                    (epoch + 1), train_epoch, per_epoch_ptime, rec_img_loss, rec_txt_loss, kl_loss, subtask_news_loss))\n",
    "                rec_txt_loss = 0\n",
    "                rec_img_loss = 0\n",
    "                kl_loss = 0\n",
    "                with torch.no_grad():\n",
    "#                     test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, valid_iterator, criterion, device, \"misogynous\")\n",
    "                    test_loss, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, test_iterator, criterion, device)\n",
    "                    f.write(f'Test subtask news loss: {test_loss:.3f} | Test Acc @1: {test_accuracy*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask news accuracy: {test_accuracy*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask news f1: {test_f1*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask news recall: {test_recall*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask news precision: {test_precision*100:6.2f}%\\n')\n",
    "                    \n",
    "                    \n",
    "                    acc /= num\n",
    "                    print(f'num_correct: {acc}\\n')\n",
    "                    print(f'total_num: {num}\\n')\n",
    "                    f.write(f'Training accuracy: {acc*100:6.2f}%\\n')\n",
    "                    \n",
    "                    if test_f1*100 >= f1_max:\n",
    "                        \n",
    "                        torch.save(vae.state_dict(), \"VAEmodel-weibo-p(1-p)-gated-epoch-%d.pkl\" % (epoch+1))\n",
    "                        f.write(\"Epoch [%d/%d]: test f1 on news improves, saving training results\\n\" % (epoch+1, train_epoch))\n",
    "                        f1_max = test_f1*100\n",
    "\n",
    "        f.flush()\n",
    "\n",
    "    f.write(\"Training finish!... save training results\\n\")\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    with torch.no_grad():\n",
    "        batch_size = y.shape[0]\n",
    "        _, top_pred = y_pred.topk(1, 1)\n",
    "        top_pred = top_pred.t()\n",
    "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
    "        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim = True)\n",
    "        acc_1 = correct_1 / batch_size\n",
    "    \n",
    "    top_pred = top_pred.cpu().view(batch_size)\n",
    "    y = y.cpu()\n",
    "    \n",
    "    accuracy = accuracy_score(y, top_pred)\n",
    "    #print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "    f1 = f1_score(y, top_pred)\n",
    "#     print(top_pred)\n",
    "    #print(\"f1: {}\".format(f1))\n",
    "\n",
    "    recall = recall_score(y, top_pred)\n",
    "    #print(\"recall: {}\".format(recall))\n",
    "\n",
    "    precision = precision_score(y, top_pred)\n",
    "    #print(\"precision: {}\".format(precision))\n",
    "\n",
    "    cm = confusion_matrix(y, top_pred)\n",
    "    #print(\"cm: {}\".format(cm))\n",
    "    return acc_1, accuracy, f1, recall, precision, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device, subtask_name=\"news\"):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in tqdm(enumerate(iterator, 0), desc='iterations'):\n",
    "\n",
    "            x_img = data['news_image']\n",
    "            x_img = x_img.to(device)\n",
    "            \n",
    "            x_txt = data['news_text']\n",
    "            x_txt = x_txt.to(device)\n",
    "            \n",
    "            \n",
    "            y = data['label']\n",
    "            batch_size = y.shape[0]\n",
    "            \n",
    "            x_img, x_txt = x_img.to(device), x_txt.to(device)\n",
    "\n",
    "            _, _, _, _, y_pred, gate = model(x_img, x_txt)\n",
    "                \n",
    "                \n",
    "            y_true_all += y.tolist()\n",
    "            _, top_pred = y_pred[\"news\"].topk(1, 1)\n",
    "            top_pred = top_pred.t()\n",
    "            top_pred = top_pred.cpu().view(batch_size)\n",
    "            y_pred_all += top_pred.tolist()\n",
    "            \n",
    "            y = y.to(device)\n",
    "            loss = criterion(y_pred[\"news\"], y)\n",
    "            epoch_loss += loss.item()\n",
    "                \n",
    "                \n",
    "    epoch_accuracy = accuracy_score(y_true_all, y_pred_all)\n",
    "    #print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "    epoch_f1 = f1_score(y_true_all, y_pred_all)\n",
    "#     print(top_pred)\n",
    "    #print(\"f1: {}\".format(f1))\n",
    "\n",
    "    epoch_recall = recall_score(y_true_all, y_pred_all)\n",
    "    #print(\"recall: {}\".format(recall))\n",
    "\n",
    "    epoch_precision = precision_score(y_true_all, y_pred_all)\n",
    "    #print(\"precision: {}\".format(precision))\n",
    "                \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_accuracy, epoch_f1, epoch_recall, epoch_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = main()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, device):\n",
    "    \n",
    "    name_dict = dict()\n",
    "    name_dict[\"news\"] = 0\n",
    "    \n",
    "    y_test = dict()\n",
    "    y_test[\"news\"] = []\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in tqdm(enumerate(iterator, 0), desc='iterations'):\n",
    "\n",
    "            x_img = data['news_image']\n",
    "            x_img = x_img.to(device)\n",
    "            \n",
    "            x_txt = data['news_text']\n",
    "            \n",
    "            x_img, x_txt = x_img.to(device), x_txt.to(device)\n",
    "\n",
    "            _, _, _, _, y_pred, gated_score = model(x_img, x_txt)\n",
    "            scores.append(gated_score.squeeze(axis=1))\n",
    "            \n",
    "            \n",
    "            for subtask_name, subtask_index in name_dict.items():\n",
    "                subtask_y = y_pred[subtask_name].cpu()\n",
    "                for dp in subtask_y:\n",
    "                    if dp[0] >= dp[1]:\n",
    "                        y_test[subtask_name].append(0)\n",
    "                    else:\n",
    "                        y_test[subtask_name].append(1)\n",
    "        \n",
    "    return y_test, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364586f049b04e4dbf82c70ba8d731a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "iterations: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test, scores = test(best_VAE, test_iterator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
